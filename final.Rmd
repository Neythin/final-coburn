---
title: "Valorant Win Percentage Prediction Model"
author: "Nathan De Los Santos"
date: "Fall 2022"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


![](images/valorant.jpeg)

# Introduction

The aim of this project is to be able to predict a player's win percentage based on their individual performance.

### What is Valorant?

Valorant is a free-to-play, first-person, tactical shooter published by Riot Games. Although officially released in June 2020, the development of the game started in 2014 and did not start their closed beta access until April 2020. Valorant is set in the near future, where players play as one of 20 "agents" -- characters based on several countries and cultures around the world. In the main game mode, players are assigned to a team of 5, either attacking or defending, with each team aiming to be the first to win 13 rounds. Not only is there a halftime to switch sides, but there is an overtime if the teams end up in a tie. Agents have unique abilities, each requiring charges, as well as a unique ultimate ability that requires charging through kills, deaths, orbs, or objectives. 

### Why is this project relevant?

As this game is still relatively young, it can easily be seen that this game is also rapidly changing. Due to factors such as new maps, new agents, or updates that can buff or nerf certain aspects of this game, the meta of Valorant is ever-changing. Moreover, as Valorant is establishing its name as the next up and coming e-sport, professional players are dissecting every bit of the game to get the edge over their opponents. Through this project, we can do a deep dive of which factors lead to a higher win percentage and what trends are currently taking over.  

### Project Timeline

With all of that settled, let's cover exactly how we're going to accomplish this. First comes some data clean up and manipulation. Secondly, we'll get into some exploratory data analysis -- seeing the distribution of our outcome variable "win_percent" and the relationships among the many predictor variables. Next, we'll follow that up with our models, specifically linear regression, lasso regression, decision forest, and random forest. After fitting a training set to these models, we will fit the best performing model to the testing data. 

Now that we all have a basic understanding of the game, let's get started!

### Loading Libraries and Data
```{r, warning=FALSE, message=FALSE}

library(tidymodels)
library(tidyverse)
library(ISLR)
library(ISLR2)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR)
library(ggplot2)
library(ggthemes)
library(pROC)
library(janitor)
library(corrplot)
library(rpart.plot)
library(vip)
library(randomForest)
library(xgboost)
library(yardstick)
tidymodels_prefer()

# These are all the libraries I used to make this project happen!
```



# Data Cleaning

### Setting up the data

Before we can even modify the data set, we have to read it in! Although the data was already pretty organized and neat, let's use the clean_names function to ensure it.
```{r}
valData <- read.csv('Data/val_stats.csv')

valorant <- valData %>% clean_names()
```

Let's remove the predictor variables "region", "name", and "tag" as they are just characteristics to distinguish the players and not so relevant with predicting win percentage.
```{r}
valorant <- valorant %>% 
  select(-region, -name, -tag, -agent_1)
```

We'll remove the extremities, as they were invalid entries due to either incorrect input or missing an input in general.
```{r}
valorant <- valorant %>% 
  filter(win_percent != 0, 
           win_percent != 100,
         rating == 'Unrated' | rating == 'Bronze 2' | rating == 'Bronze 3' |
         rating == 'Silver 1' | rating == 'Silver 2' | rating == 'Silver 3' |
         rating == 'Gold 1' | rating == 'Gold 2' | rating == 'Gold 3' |
         rating == 'Platinum 1' | rating == 'Platinum 2' | rating == 'Platinum 3' |
         rating == 'Diamond 1' | rating == 'Diamond 2' | rating == 'Diamond 3' |
         rating == 'Immortal 1' | rating == 'Immortal 2' | rating == 'Immortal 3' | 
           rating == 'Radiant')
```

Here we turn all the nominal variables into factors so that they can be applied to our models.
```{r}
#valorant$agent_1 <- factor(valorant$agent_1)
valorant$agent_2 <- factor(valorant$agent_2)
valorant$agent_3 <- factor(valorant$agent_3)

valorant$gun1_name <- factor(valorant$gun1_name)
valorant$gun2_name <- factor(valorant$gun2_name)
valorant$gun3_name <- factor(valorant$gun3_name)
```

Same thing for the "rating" variable, and let's organize the ranks to match the layout of the game.
```{r}
valorant$rating <- factor(valorant$rating,
                          levels = c('Unrated', 'Bronze 2', 'Bronze 3',
                                     'Silver 1', 'Silver 2', 'Silver 3',
                                     'Gold 1', 'Gold 2', 'Gold 3',
                                     'Platinum 1', 'Platinum 2', 'Platinum 3',
                                     'Diamond 1', 'Diamond 2', 'Diamond 3',
                                     'Immortal 1', 'Immortal 2', 'Immortal 3', 
                                     'Radiant'))
```

These variables were read in as character variables due to the comma in the number, so let's set them to their appropriate class.
```{r}
valorant$gun1_kills <- as.numeric(gsub(',', '', valorant$gun1_kills))
valorant$gun2_kills <- as.numeric(gsub(',', '', valorant$gun2_kills))
valorant$first_bloods <- as.numeric(gsub(',', '', valorant$first_bloods))
valorant$kills <- as.numeric(gsub(',', '', valorant$kills))
valorant$deaths <- as.numeric(gsub(',', '', valorant$deaths))
valorant$assists <- as.numeric(gsub(',', '', valorant$assists))
valorant$headshots <- as.numeric(gsub(',', '', valorant$headshots))
```

That does it for the data clean up! Let's set the size to lessen the run-time and set the seed to get consistent results!
```{r}
valorant <- sample_n(valorant, size = 150)

set.seed(1)
```



# Exploratory Data Analysis

### Viewing the Data

To start of the EDA, we'll take a quick look at the data to see if it's in the format that we like.
```{r}
head(valorant)
```
Everything looks good here!

```{r}
dim(valorant)
```
Being that we started with 38 total variables and then removed "region", "name", and "tag", we correctly have 35 variables left; and we can see that we have the correct amount of observations specified in our sample_n function! Let's see how well the numerical variables correlate with each other...

```{r}
# Correlation plot

valorant %>% 
  select(where(is.numeric)) %>% 
  cor() %>% 
  corrplot(type = 'full', diag = FALSE, method = 'square', order = 'AOE',
           tl.col = 'orange', col = COL2('PuOr'))
```
As we can see, majority of the variables actually go hand-in-hand with one another, with the exception of some negative correlations with gun1_head and gun1_body for example; but that's to be expected. To no surprise, kills are highly correlated with variables like "clutches", "flawless", and "deaths"; but it came as a shock to me to see that there was actually a negative correlation with variables such as "gun2_head" and "win_percent"! Speaking of "win_percent", let's inspect the distribution for it -- being that it's the variable we're predicting.

```{r}
# Histogram

valorant %>% 
  ggplot(aes(win_percent)) +
  geom_histogram(bins = 50)
```
It can be observed that our response variable, "win_percent", has a skewed right distribution -- with majority of the players' win percentages being around 50%. We have a couple of players winning a very high amount of their games this act, sitting at around the 90% area. Yet, we also have a bunch of players that are not doing so hot, sitting close to the 20% area.

```{r}
# Bar Plots

valorant %>% 
  ggplot(aes(y = agent_1)) +
  geom_bar()

valorant %>% 
  ggplot(aes(y = gun1_name)) +
  geom_bar()

valorant %>% 
  ggplot(aes(y = gun2_name)) +
  geom_bar()

valorant %>% 
  ggplot(aes(y = gun3_name)) +
  geom_bar()
```

```{r}
# Box and Whisker Plot

valorant %>% 
  ggplot(aes(x = score_round, y = gun3_name)) +
  geom_boxplot()

valorant %>% 
  ggplot(aes(x = headshot_percent, y = rating)) +
  geom_boxplot()

valorant %>% 
  ggplot(aes(x = damage_round, y = gun1_name)) +
  geom_boxplot()
```



# Model Building

### Data Split
```{r}
valSplit <- initial_split(valorant, prop = 0.8, strata = win_percent)
valTrain <- training(valSplit)
valTest <- testing(valSplit)
```

### Folding the Data
```{r}
valFold <- vfold_cv(valTrain, strata = win_percent, v = 5)
```

### Creating the Recipe
```{r}
valRecipe <- recipe(win_percent ~ ., data = valTrain) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_other(agent_1, agent_2, agent_3, gun1_name, gun2_name, gun3_name) %>% 
  step_novel(agent_1, agent_2, agent_3, gun1_name, gun2_name, gun3_name) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_pca(headshots, most_kills, wins, kd_ratio,
         gun1_head, gun1_body, gun1_legs,
         gun2_head, gun2_body, gun2_legs,
         gun3_head, gun3_body, gun3_legs,
         gun1_kills, gun2_kills, gun3_kills,
         kills, assists, deaths, 
         headshot_percent, win_percent,
         flawless, clutches, aces, first_bloods,
         score_round, damage_round, kills_round,
         num_comp = 3) %>% 
  step_nzv(all_predictors())
```

```{r}
# rating + damage_round + headshots + headshot_percent + aces + clutches + flawless + first_bloods + kills + deaths + assists + kd_ratio + kills_round + most_kills + score_round + wins + win_percent + agent_1 + agent_2 + agent_3 + gun1_name + gun1_head + gun1_body + gun1_legs + gun1_kills + gun2_name + gun2_head + gun2_body + gun2_legs + gun2_kills + gun3_name + gun3_head + gun3_body + gun3_legs + gun3_kills
```

### Model Fits
```{r}
# Logistic Regression
lm_model <- linear_reg() %>% 
  set_engine('lm')

lm_wflow <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(valRecipe)

lm_fit <- fit_resamples(lm_wflow, valFold)
```



```{r}
# Lasso Regression
lasso_spec <- linear_reg(mixture = 1, penalty = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet") 

lasso_workflow <- workflow() %>% 
  add_recipe(valRecipe) %>% 
  add_model(lasso_spec)

penalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)

tune_res <- tune_grid(lasso_workflow, resamples = valFold, grid = penalty_grid)
```

```{r}
autoplot(tune_res)

best_penalty <- select_best(tune_res, metric = "rsq")

lasso_final <- finalize_workflow(lasso_workflow, best_penalty)

lasso_final_fit <- fit(lasso_final, data = valTrain)
```



```{r}
# Decision Tree
reg_tree_spec <- tree_spec %>%
  set_mode("regression")

reg_tree_fit <- fit(reg_tree_spec, win_percent ~ .)
```



```{r}
# Random Forest
rf_spec <- rand_forest(mtry = 6) %>%
  set_engine("randomForest", importance = TRUE) %>%
  set_mode("regression")

rf_fit <- fit(rf_spec, win_percent ~ ., data = valTrain)
```


### Logistic Regression Metrics
```{r}
#valorant_train_res <- predict(lm_fit, new_data = valTrain %>% select(win_percent))

#lm_metrics <- metric_set(rmse, rsq, mae)
#lm_train_res <- predict(lm_fit, new_data = valTrain)
#lm_train_res <- bind_cols(lm_train_res, valTrain)
#lm_metrics(lm_train_res, truth = win_percent, estimate = .pred)
```

### Lasso Regression Metrics
```{r}
#augment(ridge_final_fit, new_data = valTest) %>%
#  rsq(truth = win_percent, estimate = .pred)
```

### Decision Tree Metrics
```{r}

```

### Random Forest Metrics
```{r}

```



# Conclusion
```{r}

```